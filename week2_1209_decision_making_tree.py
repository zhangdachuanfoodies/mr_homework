
# coding: utf-8

# ## Week 2 dexision-making tree

# ## 决策树模型的原理

# #### 优点：计算复杂度不高，输出结果易于理解，对中间值确实不敏感，可以处理不相关特征数据
# #### 缺点：可能会产生过度匹配问题
# #### 适用数据类型： 数值型和标称型
# 
# #### 原理：依据信息增益最大原则（通过香农熵或者基尼不纯度评估），寻找划分数据集的最好特征，使用ID3或二分法等划分数据集，之后递归的划分数据集，递归结束的条件是程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。

# ## 哪个属性对应的信息增益最大

# #### 特征A对训练数据集D的信息增益g(D, A)， 定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差。
# #### g(D,A) = H(D) - H(D|A)
# 
# #### H = - [i=1,n]p(xi)log2p(xi)
# #### for key in labelCounts:
# ####        prob = float(labelCounts[key]/numEntries)
# ####        shannonEnt -= prob * log(prob, 2)
# 

# In[9]:

from numpy import *
# 图一
# 经验熵
HD = -9/14*log(9/14) - 5/14*log(5/14)
# 熵  =  所有的   期望*信息   之和

# 经验条件熵
# 划分数据集之后（条件下）的熵
HDA = 5/14 * (-2/5*log(2/5) - 3/5*log(3/5)) + 4/14*(-1*log(1) - 0) + 5/14 * (-3/5*log(3/5) -2/5*log(2/5))

HD - HDA


# In[18]:

# 图二
HD = -9/14*log(9/14) - 5/14*log(5/14)
HD

HDA = 1/2*(-3/7*log(3/7) - 4/7*log(4/7)) + 1/2*(-6/7*log(6/7) - 1/7*log(1/7))
HDA 

HD - HDA


# In[24]:

# 图三
HD = -9/14*log(9/14) - 5/14*log(5/14)
HD
HDA = 8/14*(-6/8*log(6/8) - 2/8*log(2/8)) + 6/14*(-1/2*log(1/2) - 1/2*log(1/2))
HDA

HD - HDA


# In[31]:

# 图四
HD = -9/14*log(9/14) - 5/14*log(5/14)
HD

HDA = 4/14*(-1/2*log(1/2)-1/2*log(1/2)) + 6/14*(-2/3*log(2/3) - 1/3*log(1/3)) +      4/14*(-3/4*log(3/4) - 1/4*log(1/4))
HDA
HD - HDA


# In[ ]:



